{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaA4ccjTR4uV",
        "outputId": "ec23786d-181d-463f-e4f4-218b31eadc14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'dataset': No such file or directory\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=111HiEoEvZDdg1Y2EefI6n5dA_p4sMV4V\n",
            "From (redirected): https://drive.google.com/uc?id=111HiEoEvZDdg1Y2EefI6n5dA_p4sMV4V&confirm=t&uuid=8de90087-d64b-4d1e-b2c8-fd5aca440cc8\n",
            "To: /content/imagenet-a.tar\n",
            "100% 688M/688M [00:05<00:00, 123MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Init workspace\n",
        "!rm -r dataset\n",
        "!mkdir dataset\n",
        "\n",
        "# Download dataset and extract it\n",
        "!gdown 111HiEoEvZDdg1Y2EefI6n5dA_p4sMV4V\n",
        "!mv imagenet-a.tar ./dataset\n",
        "!tar -xf ./dataset/imagenet-a.tar\n",
        "!mv imagenet-a ./dataset\n",
        "\n",
        "# Cleanup\n",
        "!rm ./dataset/imagenet-a.tar\n",
        "\n",
        "# (optional) Upgrading pytorch for the latest augmentation functions\n",
        "#!pip install --upgrade torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUzDPvD_e8Fq",
        "outputId": "26abfc84-ffca-4850-b7cc-ce2e26a51477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1WKQGHjHUkIwZT0P2TpU9h-lY-6CnrsDd\n",
            "From (redirected): https://drive.google.com/uc?id=1WKQGHjHUkIwZT0P2TpU9h-lY-6CnrsDd&confirm=t&uuid=9b61578a-3aae-4ed0-8156-c6203c75af93\n",
            "To: /content/imagenetv2-matched-frequency.tar.gz\n",
            "100% 1.26G/1.26G [00:09<00:00, 127MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Init workspace\n",
        "!rm -r dataset\n",
        "!mkdir dataset\n",
        "\n",
        "# Download dataset and extract it\n",
        "!gdown 1WKQGHjHUkIwZT0P2TpU9h-lY-6CnrsDd\n",
        "!mv imagenetv2-matched-frequency.tar.gz ./dataset\n",
        "!tar -xf ./dataset/imagenetv2-matched-frequency.tar.gz\n",
        "!mv imagenetv2-matched-frequency-format-val ./dataset\n",
        "\n",
        "# Cleanup\n",
        "!rm ./dataset/imagenetv2-matched-frequency.tar.gz\n",
        "\n",
        "# (optional) Upgrading pytorch for the latest augmentation functions\n",
        "#!pip install --upgrade torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ECdvhkFCpxci"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import basename, isfile, join\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from contextlib import nullcontext\n",
        "from copy import deepcopy\n",
        "from typing import Union\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import v2\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xMgaZ3ON9hNd"
      },
      "outputs": [],
      "source": [
        "# Use cuda if available\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SIZE = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pOveYdeygYdb"
      },
      "outputs": [],
      "source": [
        "def load_imagenet_v2_labels() -> list[int]:\n",
        "\n",
        "    imagenet_v2 = \"./dataset/imagenetv2-matched-frequency-format-val\"\n",
        "\n",
        "    labels = [int(f) for f in listdir(imagenet_v2) if not isfile(join(imagenet_v2, f))]\n",
        "    labels.sort()\n",
        "\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "scjsACtS7Mzz"
      },
      "outputs": [],
      "source": [
        "def load_model_labels() -> list[str]:\n",
        "\n",
        "    url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
        "    path = Path(basename(url))\n",
        "\n",
        "    # Check if labels file already exists\n",
        "    if not path.exists():\n",
        "        response = requests.get(url)\n",
        "        path.write_text(response.text)\n",
        "\n",
        "    # Load labels\n",
        "    with open(path, \"r\") as f:\n",
        "        labels = json.load(f)\n",
        "\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mZ4Mo02l8cqQ"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name: str = \"google/vit-base-patch16-224\") -> ViTForImageClassification:\n",
        "\n",
        "    # Load the pre-trained model\n",
        "    return ViTForImageClassification.from_pretrained(model_name).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X5w5Qab7nSxW"
      },
      "outputs": [],
      "source": [
        "class ImageNetV2(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = load_imagenet_v2_labels() * 10\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        label = idx // 10\n",
        "\n",
        "        img_folder = os.path.join(self.img_dir, str(label))\n",
        "        img_path = [join(img_folder, f) for f in listdir(img_folder) if isfile(join(img_folder, f))][idx % (label if label != 0 else 1) - 1]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ut7hOuKqfnct"
      },
      "outputs": [],
      "source": [
        "def load_dataset(resize: bool = True) -> torch.utils.data.dataloader.DataLoader:\n",
        "\n",
        "    imagenet_v2 = \"./dataset/imagenetv2-matched-frequency-format-val\"\n",
        "\n",
        "    # Prepare data transformations for the train loader\n",
        "    transforms = [] if not resize else [T.Resize(SIZE)]\n",
        "    transforms.append(T.ToTensor())\n",
        "    transform = T.Compose(transforms)\n",
        "\n",
        "    # Load data\n",
        "    imagenet_v2_dataset = ImageNetV2(annotations_file=[], img_dir=imagenet_v2, transform=transform)\n",
        "    return torch.utils.data.DataLoader(imagenet_v2_dataset, 1, shuffle=True, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Bk0WB8NHXXlQ"
      },
      "outputs": [],
      "source": [
        "def classify(model: ViTForImageClassification, img: torch.Tensor, no_grad: bool = True) -> dict:\n",
        "\n",
        "    # Use GPU if available\n",
        "    img = img.to(DEVICE)\n",
        "\n",
        "    # Perform inference\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad() if no_grad else nullcontext():\n",
        "        outputs = model(img)\n",
        "\n",
        "    # Extract probabilities from model's output logits\n",
        "    results = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iLeoCaOmWG2u"
      },
      "outputs": [],
      "source": [
        "def elaborate_results(results: torch.Tensor) -> Union[dict, list]:\n",
        "\n",
        "    # Load model's labels\n",
        "    model_labels = load_model_labels()\n",
        "\n",
        "    if len(results.shape) == 1:\n",
        "        results = [results]\n",
        "\n",
        "    # Process results\n",
        "    final_results = []\n",
        "\n",
        "    for result in results:\n",
        "\n",
        "        item_results = {\n",
        "            \"predicted\": {},\n",
        "            \"results\": {}\n",
        "        }\n",
        "\n",
        "        predicted = None\n",
        "\n",
        "        for index, probability in enumerate(result):\n",
        "\n",
        "            item_results[\"results\"][index] = {\n",
        "                \"index\": index,\n",
        "                \"label\": model_labels[index],\n",
        "                \"probability\": probability.item()\n",
        "            }\n",
        "\n",
        "            if predicted is None or predicted[\"probability\"] < probability.item():\n",
        "                predicted = item_results[\"results\"][index]\n",
        "\n",
        "        item_results[\"predicted\"] = predicted\n",
        "\n",
        "        final_results.append(item_results)\n",
        "\n",
        "    return final_results if len(final_results) > 1 else final_results[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrXGkChUYXHS"
      },
      "outputs": [],
      "source": [
        "# # Load model (only once)\n",
        "# model = load_model()\n",
        "\n",
        "# # Load data (only once)\n",
        "# data_loader = load_dataset()\n",
        "\n",
        "# # Evaluate the model (Accuracy: 18.37 %)\n",
        "# accuracy = 0\n",
        "\n",
        "# for index, img in enumerate(data_loader):\n",
        "\n",
        "#     # Get model prediction\n",
        "#     results = classify(model=model, img=img[0])\n",
        "#     results = elaborate_results(results=results)\n",
        "#     predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "#     if img[1].item() == predicted[\"index\"]:\n",
        "#         accuracy = accuracy + 1\n",
        "\n",
        "#     print(f\"Image {index+1} / {len(data_loader)} | Accuracy: {round((accuracy / (index + 1)) * 100, 2)}% ({accuracy} / {index + 1})\")\n",
        "\n",
        "# accuracy = accuracy / len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def show_image(img):\n",
        "  plt.imshow(img.squeeze(0).permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "rpjtSheX9NJo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "AUGMENTATIONS\n",
        "\"\"\"\n",
        "\n",
        "import skimage as sk\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from PIL import Image as PILImage\n",
        "import cv2\n",
        "from scipy.ndimage import zoom as scizoom\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "\"\"\"helper for zoom_blur\"\"\"\n",
        "def clipped_zoom(img, zoom_factor):\n",
        "    h = img.shape[0]\n",
        "    # ceil crop height(= crop width)\n",
        "    ch = int(np.ceil(h / zoom_factor))\n",
        "\n",
        "    top = (h - ch) // 2\n",
        "    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n",
        "    # trim off any extra pixels\n",
        "    trim_top = (img.shape[0] - h) // 2\n",
        "\n",
        "    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n",
        "\n",
        "\"\"\"helper for defocus_blur\"\"\"\n",
        "def disk(radius, alias_blur=0.1, dtype=np.float32):\n",
        "    if radius <= 8:\n",
        "        L = np.arange(-8, 8 + 1)\n",
        "        ksize = (3, 3)\n",
        "    else:\n",
        "        L = np.arange(-radius, radius + 1)\n",
        "        ksize = (5, 5)\n",
        "    X, Y = np.meshgrid(L, L)\n",
        "    aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n",
        "    aliased_disk /= np.sum(aliased_disk)\n",
        "\n",
        "    # supersample disk to antialias\n",
        "    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n",
        "\n",
        "def saturate(x, severity=1):\n",
        "    c = [(0.3, 0), (0.1, 0), (2, 0), (5, 0.1), (20, 0.2)][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    x = sk.color.rgb2hsv(x)\n",
        "    x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n",
        "    x = sk.color.hsv2rgb(x)\n",
        "\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "\n",
        "def pixelate(x, severity=1):\n",
        "    c = [0.6, 0.5, 0.4, 0.3, 0.25][severity - 1]\n",
        "\n",
        "    x = x.resize((int(IMG_SIZE * c), int(IMG_SIZE * c)), PILImage.BOX)\n",
        "    x = x.resize((IMG_SIZE, IMG_SIZE), PILImage.BOX)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def jpeg_compression(x, severity=1):\n",
        "    c = [25, 18, 15, 10, 7][severity - 1]\n",
        "\n",
        "    output = BytesIO()\n",
        "    x.save(output, 'JPEG', quality=c)\n",
        "    x = PILImage.open(output)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def brightness(x, severity=1):\n",
        "    c = [.1, .2, .3, .4, .5][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    x = sk.color.rgb2hsv(x)\n",
        "    x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n",
        "    x = sk.color.hsv2rgb(x)\n",
        "\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "\n",
        "def zoom_blur(x, severity=1):\n",
        "    c = [np.arange(1, 1.11, 0.01),\n",
        "         np.arange(1, 1.16, 0.01),\n",
        "         np.arange(1, 1.21, 0.02),\n",
        "         np.arange(1, 1.26, 0.02),\n",
        "         np.arange(1, 1.31, 0.03)][severity - 1]\n",
        "\n",
        "    x = (np.array(x) / 255.).astype(np.float32)\n",
        "    out = np.zeros_like(x)\n",
        "    for zoom_factor in c:\n",
        "        zoomed = clipped_zoom(x, zoom_factor)\n",
        "        resized_zoomed = cv2.resize(zoomed, (out.shape[1], out.shape[0]))\n",
        "        out += resized_zoomed\n",
        "\n",
        "    x = (x + out) / (len(c) + 1)\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "\n",
        "def defocus_blur(x, severity=1):\n",
        "    c = [(3, 0.1), (4, 0.5), (6, 0.5), (8, 0.5), (10, 0.5)][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    kernel = disk(radius=c[0], alias_blur=c[1])\n",
        "\n",
        "    channels = []\n",
        "    for d in range(3):\n",
        "        channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n",
        "    channels = np.array(channels).transpose((1, 2, 0))  # 3x224x224 -> 224x224x3\n",
        "\n",
        "    return np.clip(channels, 0, 1) * 255\n",
        "\n",
        "\n",
        "def gaussian_noise(x, severity=1):\n",
        "    c = [.08, .12, 0.18, 0.26, 0.38][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n",
        "\n",
        "\n",
        "def shot_noise(x, severity=1):\n",
        "    c = [60, 25, 12, 5, 3][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    return np.clip(np.random.poisson(x * c) / c, 0, 1) * 255\n",
        "\n",
        "def impulse_noise(x, severity=1):\n",
        "    c = [.03, .06, .09, 0.17, 0.27][severity - 1]\n",
        "\n",
        "    x = sk.util.random_noise(np.array(x) / 255., mode='s&p', amount=c)\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "def speckle_noise(x, severity=1):\n",
        "    c = [.15, .2, 0.35, 0.45, 0.6][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    return np.clip(x + x * np.random.normal(size=x.shape, scale=c), 0, 1) * 255"
      ],
      "metadata": {
        "id": "4J0hHXdRfuOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(img, augmentation_function, severity):\n",
        "  img = img * 255\n",
        "  img = img.to(torch.uint8)\n",
        "\n",
        "  img_np = img.numpy().transpose(0, 2, 3, 1)\n",
        "  augmented_np = augmentation_function(img_np, severity=severity)\n",
        "\n",
        "  augmented_tensor = torch.tensor(augmented_np, dtype=torch.uint8).permute(0, 3, 1, 2)\n",
        "  return augmented_tensor"
      ],
      "metadata": {
        "id": "NUkOrsxSf4mT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# crea un batch di n immagini, in cui ognugna è una versione augmented dell'immagine\n",
        "# se n < numero augmentations allora ne prende una diversa ogni volta, altrimetni random\n",
        "\n",
        "def create_batch(img, n):\n",
        "  augmentations = [saturate, brightness, gaussian_noise, shot_noise, impulse_noise, speckle_noise]\n",
        "\n",
        "  a = set()\n",
        "  augmented = []\n",
        "  while len(augmented) < n:\n",
        "    i = random.randint(0, len(augmentations)-1)\n",
        "    if len(augmented) == len(augmentations):\n",
        "      a.clear()\n",
        "    if i in a: continue\n",
        "    a.add(i)\n",
        "    augmented_img = augment(img, augmentations[i], severity=2)\n",
        "    augmented.append(augmented_img)\n",
        "  return torch.cat(augmented, dim=0)"
      ],
      "metadata": {
        "id": "-KFdzo1qjOxc"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IzaEQC-_dzji",
        "outputId": "05840bcf-e404-463f-d612-4539ef777e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1 / 10000 | Accuracy before: 100.0% (1 / 1) | Accuracy after: 100.0% (1 / 1) | Diff: 0.0%\n",
            "Image 2 / 10000 | Accuracy before: 100.0% (2 / 2) | Accuracy after: 100.0% (2 / 2) | Diff: 0.0%\n",
            "Image 3 / 10000 | Accuracy before: 100.0% (3 / 3) | Accuracy after: 100.0% (3 / 3) | Diff: 0.0%\n",
            "Image 4 / 10000 | Accuracy before: 100.0% (4 / 4) | Accuracy after: 100.0% (4 / 4) | Diff: 0.0%\n",
            "Image 5 / 10000 | Accuracy before: 100.0% (5 / 5) | Accuracy after: 100.0% (5 / 5) | Diff: 0.0%\n",
            "Image 6 / 10000 | Accuracy before: 100.0% (6 / 6) | Accuracy after: 100.0% (6 / 6) | Diff: 0.0%\n",
            "Image 7 / 10000 | Accuracy before: 85.7% (6 / 7) | Accuracy after: 85.7% (6 / 7) | Diff: 0.0%\n",
            "Image 8 / 10000 | Accuracy before: 75.0% (6 / 8) | Accuracy after: 87.5% (7 / 8) | Diff: 12.5%\n",
            "Image 9 / 10000 | Accuracy before: 66.7% (6 / 9) | Accuracy after: 77.8% (7 / 9) | Diff: 11.11%\n",
            "Image 10 / 10000 | Accuracy before: 70.0% (7 / 10) | Accuracy after: 80.0% (8 / 10) | Diff: 10.0%\n",
            "Image 11 / 10000 | Accuracy before: 72.7% (8 / 11) | Accuracy after: 81.8% (9 / 11) | Diff: 9.09%\n",
            "Image 12 / 10000 | Accuracy before: 75.0% (9 / 12) | Accuracy after: 83.3% (10 / 12) | Diff: 8.33%\n",
            "Image 13 / 10000 | Accuracy before: 76.9% (10 / 13) | Accuracy after: 84.6% (11 / 13) | Diff: 7.69%\n",
            "Image 14 / 10000 | Accuracy before: 78.6% (11 / 14) | Accuracy after: 85.7% (12 / 14) | Diff: 7.14%\n",
            "Image 15 / 10000 | Accuracy before: 73.3% (11 / 15) | Accuracy after: 80.0% (12 / 15) | Diff: 6.67%\n",
            "Image 16 / 10000 | Accuracy before: 75.0% (12 / 16) | Accuracy after: 75.0% (12 / 16) | Diff: 0.0%\n",
            "Image 17 / 10000 | Accuracy before: 76.5% (13 / 17) | Accuracy after: 76.5% (13 / 17) | Diff: 0.0%\n",
            "Image 18 / 10000 | Accuracy before: 77.8% (14 / 18) | Accuracy after: 77.8% (14 / 18) | Diff: 0.0%\n",
            "Image 19 / 10000 | Accuracy before: 78.9% (15 / 19) | Accuracy after: 78.9% (15 / 19) | Diff: 0.0%\n",
            "Image 20 / 10000 | Accuracy before: 80.0% (16 / 20) | Accuracy after: 80.0% (16 / 20) | Diff: 0.0%\n",
            "Image 21 / 10000 | Accuracy before: 81.0% (17 / 21) | Accuracy after: 81.0% (17 / 21) | Diff: 0.0%\n",
            "Image 22 / 10000 | Accuracy before: 77.3% (17 / 22) | Accuracy after: 77.3% (17 / 22) | Diff: 0.0%\n",
            "Image 23 / 10000 | Accuracy before: 73.9% (17 / 23) | Accuracy after: 73.9% (17 / 23) | Diff: 0.0%\n",
            "Image 24 / 10000 | Accuracy before: 70.8% (17 / 24) | Accuracy after: 75.0% (18 / 24) | Diff: 4.17%\n",
            "Image 25 / 10000 | Accuracy before: 72.0% (18 / 25) | Accuracy after: 76.0% (19 / 25) | Diff: 4.0%\n",
            "Image 26 / 10000 | Accuracy before: 69.2% (18 / 26) | Accuracy after: 73.1% (19 / 26) | Diff: 3.85%\n",
            "Image 27 / 10000 | Accuracy before: 66.7% (18 / 27) | Accuracy after: 70.4% (19 / 27) | Diff: 3.7%\n",
            "Image 28 / 10000 | Accuracy before: 64.3% (18 / 28) | Accuracy after: 67.9% (19 / 28) | Diff: 3.57%\n",
            "Image 29 / 10000 | Accuracy before: 65.5% (19 / 29) | Accuracy after: 69.0% (20 / 29) | Diff: 3.45%\n",
            "Image 30 / 10000 | Accuracy before: 66.7% (20 / 30) | Accuracy after: 70.0% (21 / 30) | Diff: 3.33%\n",
            "Image 31 / 10000 | Accuracy before: 64.5% (20 / 31) | Accuracy after: 67.7% (21 / 31) | Diff: 3.23%\n",
            "Image 32 / 10000 | Accuracy before: 65.6% (21 / 32) | Accuracy after: 68.8% (22 / 32) | Diff: 3.12%\n",
            "Image 33 / 10000 | Accuracy before: 66.7% (22 / 33) | Accuracy after: 69.7% (23 / 33) | Diff: 3.03%\n",
            "Image 34 / 10000 | Accuracy before: 67.6% (23 / 34) | Accuracy after: 70.6% (24 / 34) | Diff: 2.94%\n",
            "Image 35 / 10000 | Accuracy before: 68.6% (24 / 35) | Accuracy after: 71.4% (25 / 35) | Diff: 2.86%\n",
            "Image 36 / 10000 | Accuracy before: 66.7% (24 / 36) | Accuracy after: 69.4% (25 / 36) | Diff: 2.78%\n",
            "Image 37 / 10000 | Accuracy before: 67.6% (25 / 37) | Accuracy after: 70.3% (26 / 37) | Diff: 2.7%\n",
            "Image 38 / 10000 | Accuracy before: 68.4% (26 / 38) | Accuracy after: 71.1% (27 / 38) | Diff: 2.63%\n",
            "Image 39 / 10000 | Accuracy before: 69.2% (27 / 39) | Accuracy after: 71.8% (28 / 39) | Diff: 2.56%\n",
            "Image 40 / 10000 | Accuracy before: 70.0% (28 / 40) | Accuracy after: 72.5% (29 / 40) | Diff: 2.5%\n",
            "Image 41 / 10000 | Accuracy before: 70.7% (29 / 41) | Accuracy after: 73.2% (30 / 41) | Diff: 2.44%\n",
            "Image 42 / 10000 | Accuracy before: 69.0% (29 / 42) | Accuracy after: 73.8% (31 / 42) | Diff: 4.76%\n",
            "Image 43 / 10000 | Accuracy before: 67.4% (29 / 43) | Accuracy after: 72.1% (31 / 43) | Diff: 4.65%\n",
            "Image 44 / 10000 | Accuracy before: 68.2% (30 / 44) | Accuracy after: 72.7% (32 / 44) | Diff: 4.55%\n",
            "Image 45 / 10000 | Accuracy before: 66.7% (30 / 45) | Accuracy after: 71.1% (32 / 45) | Diff: 4.44%\n",
            "Image 46 / 10000 | Accuracy before: 67.4% (31 / 46) | Accuracy after: 71.7% (33 / 46) | Diff: 4.35%\n",
            "Image 47 / 10000 | Accuracy before: 66.0% (31 / 47) | Accuracy after: 70.2% (33 / 47) | Diff: 4.26%\n",
            "Image 48 / 10000 | Accuracy before: 66.7% (32 / 48) | Accuracy after: 70.8% (34 / 48) | Diff: 4.17%\n",
            "Image 49 / 10000 | Accuracy before: 65.3% (32 / 49) | Accuracy after: 69.4% (34 / 49) | Diff: 4.08%\n",
            "Image 50 / 10000 | Accuracy before: 66.0% (33 / 50) | Accuracy after: 70.0% (35 / 50) | Diff: 4.0%\n",
            "Image 51 / 10000 | Accuracy before: 66.7% (34 / 51) | Accuracy after: 70.6% (36 / 51) | Diff: 3.92%\n",
            "Image 52 / 10000 | Accuracy before: 65.4% (34 / 52) | Accuracy after: 69.2% (36 / 52) | Diff: 3.85%\n",
            "Image 53 / 10000 | Accuracy before: 66.0% (35 / 53) | Accuracy after: 69.8% (37 / 53) | Diff: 3.77%\n",
            "Image 54 / 10000 | Accuracy before: 66.7% (36 / 54) | Accuracy after: 70.4% (38 / 54) | Diff: 3.7%\n",
            "Image 55 / 10000 | Accuracy before: 67.3% (37 / 55) | Accuracy after: 70.9% (39 / 55) | Diff: 3.64%\n",
            "Image 56 / 10000 | Accuracy before: 66.1% (37 / 56) | Accuracy after: 69.6% (39 / 56) | Diff: 3.57%\n",
            "Image 57 / 10000 | Accuracy before: 66.7% (38 / 57) | Accuracy after: 68.4% (39 / 57) | Diff: 1.75%\n",
            "Image 58 / 10000 | Accuracy before: 67.2% (39 / 58) | Accuracy after: 69.0% (40 / 58) | Diff: 1.72%\n",
            "Image 59 / 10000 | Accuracy before: 67.8% (40 / 59) | Accuracy after: 69.5% (41 / 59) | Diff: 1.69%\n",
            "Image 60 / 10000 | Accuracy before: 66.7% (40 / 60) | Accuracy after: 68.3% (41 / 60) | Diff: 1.67%\n",
            "Image 61 / 10000 | Accuracy before: 65.6% (40 / 61) | Accuracy after: 67.2% (41 / 61) | Diff: 1.64%\n",
            "Image 62 / 10000 | Accuracy before: 66.1% (41 / 62) | Accuracy after: 67.7% (42 / 62) | Diff: 1.61%\n",
            "Image 63 / 10000 | Accuracy before: 65.1% (41 / 63) | Accuracy after: 66.7% (42 / 63) | Diff: 1.59%\n",
            "Image 64 / 10000 | Accuracy before: 64.1% (41 / 64) | Accuracy after: 67.2% (43 / 64) | Diff: 3.12%\n",
            "Image 65 / 10000 | Accuracy before: 63.1% (41 / 65) | Accuracy after: 66.2% (43 / 65) | Diff: 3.08%\n",
            "Image 66 / 10000 | Accuracy before: 63.6% (42 / 66) | Accuracy after: 66.7% (44 / 66) | Diff: 3.03%\n",
            "Image 67 / 10000 | Accuracy before: 64.2% (43 / 67) | Accuracy after: 67.2% (45 / 67) | Diff: 2.99%\n",
            "Image 68 / 10000 | Accuracy before: 63.2% (43 / 68) | Accuracy after: 66.2% (45 / 68) | Diff: 2.94%\n",
            "Image 69 / 10000 | Accuracy before: 63.8% (44 / 69) | Accuracy after: 66.7% (46 / 69) | Diff: 2.9%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-b911b434328d>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Immagine ridimensionata (384x384)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_transformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-c51fa937229c>\u001b[0m in \u001b[0;36mcreate_batch\u001b[0;34m(img, n)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0maugmented_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseverity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0maugmented\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-d15ce8ceaddc>\u001b[0m in \u001b[0;36maugment\u001b[0;34m(img, augmentation_function, severity)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mimg_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0maugmented_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseverity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseverity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0maugmented_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-2921db83984a>\u001b[0m in \u001b[0;36mshot_noise\u001b[0;34m(x, severity)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoisson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimpulse_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseverity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.poisson\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.int64_to_long\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36misscalar\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1855\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Supponendo che load_dataset, classify_image, merged_labels e il modello siano definiti altrove\n",
        "accuracy_before = 0\n",
        "accuracy_after = 0\n",
        "\n",
        "# Load model (only once)\n",
        "model = load_model()\n",
        "original_model = deepcopy(model)\n",
        "\n",
        "data_loader = load_dataset(resize=False)\n",
        "\n",
        "# transformation = T.Compose([\n",
        "#         T.Resize((500, 500)),\n",
        "#         T.CenterCrop((384, 384)) ])\n",
        "\n",
        "transforms = v2.Compose([\n",
        "    v2.RandomResizedCrop(size=SIZE, antialias=True),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "resize_transformation = T.Compose([ T.Resize(SIZE) ])\n",
        "\n",
        "# Salva lo stato iniziale del modello\n",
        "initial_state = model.state_dict().copy()\n",
        "\n",
        "for index, img in enumerate(data_loader):\n",
        "    # Ripristina lo stato iniziale del modello\n",
        "    model = deepcopy(original_model)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "    # Azzera i gradienti prima di calcolare i nuovi\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Immagine ridimensionata (384x384)\n",
        "    img1 = resize_transformation(img[0])\n",
        "    batch = create_batch(img[0], 5)\n",
        "    input = transforms(batch)\n",
        "\n",
        "    # Classificazione dell'immagine 1 prima delle augmentation\n",
        "    results = classify(model=model, img=img1)\n",
        "    results = elaborate_results(results=results)\n",
        "    predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "    # Aggiorna accuracy della classificazione senza augmentation\n",
        "    if img[1].item() == predicted[\"index\"]:\n",
        "        accuracy_before = accuracy_before + 1\n",
        "\n",
        "    predicted_before = predicted[\"label\"]\n",
        "\n",
        "    # Calcola gli output delle immagini\n",
        "    output = model(input.to(DEVICE))\n",
        "\n",
        "    # Combina le probabilità delle immagini\n",
        "    probabilities = torch.nn.functional.softmax(output.logits, dim=-1).squeeze().to(DEVICE)\n",
        "\n",
        "    # Calcolo entropia\n",
        "    marginal = torch.mean(probabilities, dim=0).to(DEVICE)\n",
        "    entropy = -torch.sum(marginal * torch.log(marginal)).to(DEVICE)\n",
        "    entropy.backward()\n",
        "\n",
        "    # Gradient step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Classificazione dell'immagine 1 dopo le augmentation\n",
        "    results = classify(model=model, img=img1)\n",
        "    results = elaborate_results(results=results)\n",
        "    predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "    #print(probabilities1 == probabilities2)\n",
        "    # Aggiorna accuracy della classificazione con augmentation\n",
        "    if img[1].item() == predicted[\"index\"]:\n",
        "        accuracy_after = accuracy_after + 1\n",
        "\n",
        "    label1 = f\"Image {index + 1} / {len(data_loader)}\"\n",
        "    label2 = f\"Accuracy before: {round((accuracy_before / (index + 1)) * 100, 1)}% ({accuracy_before} / {index + 1})\"\n",
        "    label3 = f\"Accuracy after: {round((accuracy_after / (index + 1)) * 100, 1)}% ({accuracy_after} / {index + 1})\"\n",
        "    label4 = f\"Diff: {round((accuracy_after / (index + 1)) * 100 - (accuracy_before / (index + 1)) * 100, 2)}%\"\n",
        "\n",
        "    print(f\"{label1} | {label2} | {label3} | {label4}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5mgJPL0opeu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}