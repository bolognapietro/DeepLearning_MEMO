{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaA4ccjTR4uV"
      },
      "outputs": [],
      "source": [
        "# Init workspace\n",
        "!rm -r dataset 2>/dev/null\n",
        "!mkdir dataset\n",
        "\n",
        "# Download dataset and extract it\n",
        "!gdown 111HiEoEvZDdg1Y2EefI6n5dA_p4sMV4V\n",
        "!mv imagenet-a.tar ./dataset\n",
        "!tar -xf ./dataset/imagenet-a.tar\n",
        "!mv imagenet-a ./dataset\n",
        "\n",
        "# Cleanup\n",
        "!rm ./dataset/imagenet-a.tar\n",
        "\n",
        "# (optional) Upgrading pytorch for the latest augmentation functions\n",
        "#!pip install --upgrade torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Init workspace\n",
        "!rm -r dataset 2>/dev/null\n",
        "!mkdir dataset\n",
        "\n",
        "# Download dataset and extract it\n",
        "!gdown 1WKQGHjHUkIwZT0P2TpU9h-lY-6CnrsDd\n",
        "!mv imagenetv2-matched-frequency.tar.gz ./dataset\n",
        "!tar -xf ./dataset/imagenetv2-matched-frequency.tar.gz\n",
        "!mv imagenetv2-matched-frequency-format-val ./dataset\n",
        "\n",
        "# Cleanup\n",
        "!rm ./dataset/imagenetv2-matched-frequency.tar.gz\n",
        "\n",
        "# (optional) Upgrading pytorch for the latest augmentation functions\n",
        "#!pip install --upgrade torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "vUzDPvD_e8Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import basename, isfile, join\n",
        "from pathlib import Path\n",
        "import re\n",
        "import requests\n",
        "from contextlib import nullcontext\n",
        "from copy import deepcopy\n",
        "from typing import Union\n",
        "from io import BytesIO\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import zoom as scizoom\n",
        "import skimage as sk\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import v2\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ECdvhkFCpxci"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xMgaZ3ON9hNd"
      },
      "outputs": [],
      "source": [
        "# Use cuda if available\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SIZE = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(img):\n",
        "  plt.imshow(img.squeeze(0).permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "_aCi9if3jF-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_imagenet_v2_labels() -> list[int]:\n",
        "\n",
        "    # Load ImageNetV2's labels\n",
        "    imagenet_v2 = \"./dataset/imagenetv2-matched-frequency-format-val\"\n",
        "\n",
        "    labels = [int(f) for f in listdir(imagenet_v2) if not isfile(join(imagenet_v2, f))]\n",
        "    labels.sort()\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "pOveYdeygYdb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "scjsACtS7Mzz"
      },
      "outputs": [],
      "source": [
        "def load_model_labels() -> list[str]:\n",
        "\n",
        "    url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
        "    path = Path(basename(url))\n",
        "\n",
        "    # Check if labels file already exists\n",
        "    if not path.exists():\n",
        "        response = requests.get(url)\n",
        "        path.write_text(response.text)\n",
        "\n",
        "    # Load labels\n",
        "    with open(path, \"r\") as f:\n",
        "        labels = json.load(f)\n",
        "\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mZ4Mo02l8cqQ"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name: str = \"google/vit-base-patch16-224\") -> ViTForImageClassification:\n",
        "\n",
        "    # Load the pre-trained model\n",
        "    return ViTForImageClassification.from_pretrained(model_name).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageNetV2(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root: str, transform=None):\n",
        "\n",
        "        images = {}\n",
        "        images_per_label = None\n",
        "\n",
        "        # Create a dict where:\n",
        "        # - Each key is a label\n",
        "        # - The content of each key are the images for that label\n",
        "\n",
        "        for label in load_imagenet_v2_labels():\n",
        "            images[label] = []\n",
        "            image_folder = join(root, str(label))\n",
        "\n",
        "            for image in listdir(image_folder):\n",
        "\n",
        "                image = join(image_folder, image)\n",
        "\n",
        "                if not isfile(image):\n",
        "                    continue\n",
        "\n",
        "                images[label].append(image)\n",
        "\n",
        "            # This is done just to be sure that the number of images is equal for each label\n",
        "            if images_per_label is not None:\n",
        "                assert images_per_label == len(images[label])\n",
        "\n",
        "            images_per_label = len(images[label])\n",
        "\n",
        "        self.images = images\n",
        "        self.images_per_label = images_per_label\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images) * self.images_per_label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # The label is the requested index divided by the number of images for each label\n",
        "        label = index // self.images_per_label\n",
        "\n",
        "        # index % (label if label != 0 else 1) - 1 is used to pick a specific image for a specific label\n",
        "        image = self.images[label][index % (label if label != 0 else 1) - 1]\n",
        "        image = Image.open(image).convert(\"RGB\")\n",
        "\n",
        "        # Apply transform (if any)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "X5w5Qab7nSxW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ut7hOuKqfnct"
      },
      "outputs": [],
      "source": [
        "def load_dataset(resize: bool = True) -> torch.utils.data.dataloader.DataLoader:\n",
        "\n",
        "    imagenet_v2 = \"./dataset/imagenetv2-matched-frequency-format-val\"\n",
        "\n",
        "    # Prepare data transformations for the train loader\n",
        "    transforms = [] if not resize else [T.Resize(SIZE)]\n",
        "    transforms.append(T.ToTensor())\n",
        "    transform = T.Compose(transforms)\n",
        "\n",
        "    # Load data\n",
        "    imagenet_v2_dataset = ImageNetV2(root=imagenet_v2, transform=transform)\n",
        "    return torch.utils.data.DataLoader(imagenet_v2_dataset, 1, shuffle=True, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "Bk0WB8NHXXlQ"
      },
      "outputs": [],
      "source": [
        "def classify(model: ViTForImageClassification, img: torch.Tensor, no_grad: bool = True) -> dict:\n",
        "\n",
        "    # Use GPU if available\n",
        "    img = img.to(DEVICE)\n",
        "\n",
        "    # Perform inference\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad() if no_grad else nullcontext():\n",
        "        outputs = model(img)\n",
        "\n",
        "    # Extract probabilities from model's output logits\n",
        "    results = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def elaborate_results(results: torch.Tensor) -> Union[dict, list]:\n",
        "\n",
        "    # Load model's labels\n",
        "    model_labels = load_model_labels()\n",
        "\n",
        "    if len(results.shape) == 1:\n",
        "        results = [results]\n",
        "\n",
        "    # Process results\n",
        "    final_results = []\n",
        "\n",
        "    for result in results:\n",
        "\n",
        "        item_results = {\n",
        "            \"predicted\": {},\n",
        "            \"results\": {}\n",
        "        }\n",
        "\n",
        "        predicted = None\n",
        "\n",
        "        for index, probability in enumerate(result):\n",
        "\n",
        "            item_results[\"results\"][index] = {\n",
        "                \"index\": index,\n",
        "                \"label\": model_labels[index],\n",
        "                \"probability\": probability.item()\n",
        "            }\n",
        "\n",
        "            if predicted is None or predicted[\"probability\"] < probability.item():\n",
        "                predicted = item_results[\"results\"][index]\n",
        "\n",
        "        item_results[\"predicted\"] = predicted\n",
        "\n",
        "        final_results.append(item_results)\n",
        "\n",
        "    return final_results if len(final_results) > 1 else final_results[0]\n"
      ],
      "metadata": {
        "id": "iLeoCaOmWG2u"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "jrXGkChUYXHS"
      },
      "outputs": [],
      "source": [
        "# # Load model (only once)\n",
        "# model = load_model()\n",
        "\n",
        "# # Load data (only once)\n",
        "# data_loader = load_dataset()\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = 0\n",
        "\n",
        "# for index, img in enumerate(data_loader):\n",
        "\n",
        "#     # Get model prediction\n",
        "#     results = classify(model=model, img=img[0])\n",
        "#     results = elaborate_results(results=results)\n",
        "#     predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "#     if img[1].item() == predicted[\"index\"]:\n",
        "#         accuracy = accuracy + 1\n",
        "\n",
        "#     print(f\"Image {index+1} / {len(data_loader)} | Accuracy: {round((accuracy / (index + 1)) * 100, 2)}% ({accuracy} / {index + 1})\")\n",
        "\n",
        "# accuracy = accuracy / len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "AUGMENTATIONS\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"helper for zoom_blur\"\"\"\n",
        "def clipped_zoom(img, zoom_factor):\n",
        "    h = img.shape[0]\n",
        "    # ceil crop height(= crop width)\n",
        "    ch = int(np.ceil(h / zoom_factor))\n",
        "\n",
        "    top = (h - ch) // 2\n",
        "    img = scizoom(img[top:top + ch, top:top + ch], (zoom_factor, zoom_factor, 1), order=1)\n",
        "    # trim off any extra pixels\n",
        "    trim_top = (img.shape[0] - h) // 2\n",
        "\n",
        "    return img[trim_top:trim_top + h, trim_top:trim_top + h]\n",
        "\n",
        "\"\"\"helper for defocus_blur\"\"\"\n",
        "def disk(radius, alias_blur=0.1, dtype=np.float32):\n",
        "    if radius <= 8:\n",
        "        L = np.arange(-8, 8 + 1)\n",
        "        ksize = (3, 3)\n",
        "    else:\n",
        "        L = np.arange(-radius, radius + 1)\n",
        "        ksize = (5, 5)\n",
        "    X, Y = np.meshgrid(L, L)\n",
        "    aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n",
        "    aliased_disk /= np.sum(aliased_disk)\n",
        "\n",
        "    # supersample disk to antialias\n",
        "    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n",
        "\n",
        "def saturate(x, severity=1):\n",
        "    c = [(0.3, 0), (0.1, 0), (2, 0), (5, 0.1), (20, 0.2)][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    x = sk.color.rgb2hsv(x)\n",
        "    x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n",
        "    x = sk.color.hsv2rgb(x)\n",
        "\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "def pixelate(x, severity=1):\n",
        "    c = [0.6, 0.5, 0.4, 0.3, 0.25][severity - 1]\n",
        "\n",
        "    x = x.resize((int(SIZE[0] * c), int(SIZE[0] * c)), PIL.BOX)\n",
        "    x = x.resize(SIZE, Image.BOX)\n",
        "\n",
        "    return x\n",
        "\n",
        "def jpeg_compression(x, severity=1):\n",
        "    c = [25, 18, 15, 10, 7][severity - 1]\n",
        "\n",
        "    output = BytesIO()\n",
        "    x.save(output, 'JPEG', quality=c)\n",
        "    x = Image.open(output)\n",
        "\n",
        "    return x\n",
        "\n",
        "def brightness(x, severity=1):\n",
        "    c = [.1, .2, .3, .4, .5][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    x = sk.color.rgb2hsv(x)\n",
        "    x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n",
        "    x = sk.color.hsv2rgb(x)\n",
        "\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "def zoom_blur(x, severity=1):\n",
        "    c = [np.arange(1, 1.11, 0.01),\n",
        "         np.arange(1, 1.16, 0.01),\n",
        "         np.arange(1, 1.21, 0.02),\n",
        "         np.arange(1, 1.26, 0.02),\n",
        "         np.arange(1, 1.31, 0.03)][severity - 1]\n",
        "\n",
        "    x = (np.array(x) / 255.).astype(np.float32)\n",
        "    out = np.zeros_like(x)\n",
        "    for zoom_factor in c:\n",
        "        zoomed = clipped_zoom(x, zoom_factor)\n",
        "        resized_zoomed = cv2.resize(zoomed, (out.shape[1], out.shape[0]))\n",
        "        out += resized_zoomed\n",
        "\n",
        "    x = (x + out) / (len(c) + 1)\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "def defocus_blur(x, severity=1):\n",
        "    c = [(3, 0.1), (4, 0.5), (6, 0.5), (8, 0.5), (10, 0.5)][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    kernel = disk(radius=c[0], alias_blur=c[1])\n",
        "\n",
        "    channels = []\n",
        "    for d in range(3):\n",
        "        channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n",
        "    channels = np.array(channels).transpose((1, 2, 0))  # 3x224x224 -> 224x224x3\n",
        "\n",
        "    return np.clip(channels, 0, 1) * 255\n",
        "\n",
        "def gaussian_noise(x, severity=1):\n",
        "    c = [.08, .12, 0.18, 0.26, 0.38][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n",
        "\n",
        "def shot_noise(x, severity=1):\n",
        "    c = [60, 25, 12, 5, 3][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    return np.clip(np.random.poisson(x * c) / c, 0, 1) * 255\n",
        "\n",
        "def impulse_noise(x, severity=1):\n",
        "    c = [.03, .06, .09, 0.17, 0.27][severity - 1]\n",
        "\n",
        "    x = sk.util.random_noise(np.array(x) / 255., mode='s&p', amount=c)\n",
        "    return np.clip(x, 0, 1) * 255\n",
        "\n",
        "def speckle_noise(x, severity=1):\n",
        "    c = [.15, .2, 0.35, 0.45, 0.6][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    return np.clip(x + x * np.random.normal(size=x.shape, scale=c), 0, 1) * 255"
      ],
      "metadata": {
        "id": "0RNGfTuAhFm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(img, augmentation_function, severity):\n",
        "    img = img * 255\n",
        "    img = img.to(torch.uint8)\n",
        "\n",
        "    img_np = img.numpy().transpose(0, 2, 3, 1)\n",
        "    augmented_np = augmentation_function(img_np, severity=severity)\n",
        "\n",
        "    augmented_tensor = torch.tensor(augmented_np, dtype=torch.uint8).permute(0, 3, 1, 2)\n",
        "    return augmented_tensor"
      ],
      "metadata": {
        "id": "6_4xCEOmhFTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "IzaEQC-_dzji",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "outputId": "9864cdc7-6f63-48bb-8bea-333b2449cf2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1 / 10000 | Accuracy before: 100.0% (1 / 1) | Accuracy after: 0.0% (0 / 1) | Diff: -100.0%\n",
            "Image 2 / 10000 | Accuracy before: 100.0% (2 / 2) | Accuracy after: 50.0% (1 / 2) | Diff: -50.0%\n",
            "Image 3 / 10000 | Accuracy before: 66.7% (2 / 3) | Accuracy after: 33.3% (1 / 3) | Diff: -33.33%\n",
            "Image 4 / 10000 | Accuracy before: 50.0% (2 / 4) | Accuracy after: 25.0% (1 / 4) | Diff: -25.0%\n",
            "Image 5 / 10000 | Accuracy before: 40.0% (2 / 5) | Accuracy after: 40.0% (2 / 5) | Diff: 0.0%\n",
            "Image 6 / 10000 | Accuracy before: 50.0% (3 / 6) | Accuracy after: 50.0% (3 / 6) | Diff: 0.0%\n",
            "Image 7 / 10000 | Accuracy before: 57.1% (4 / 7) | Accuracy after: 57.1% (4 / 7) | Diff: 0.0%\n",
            "Image 8 / 10000 | Accuracy before: 50.0% (4 / 8) | Accuracy after: 50.0% (4 / 8) | Diff: 0.0%\n",
            "Image 9 / 10000 | Accuracy before: 44.4% (4 / 9) | Accuracy after: 44.4% (4 / 9) | Diff: 0.0%\n",
            "Image 10 / 10000 | Accuracy before: 50.0% (5 / 10) | Accuracy after: 50.0% (5 / 10) | Diff: 0.0%\n",
            "Image 11 / 10000 | Accuracy before: 54.5% (6 / 11) | Accuracy after: 54.5% (6 / 11) | Diff: 0.0%\n",
            "Image 12 / 10000 | Accuracy before: 50.0% (6 / 12) | Accuracy after: 50.0% (6 / 12) | Diff: 0.0%\n",
            "Image 13 / 10000 | Accuracy before: 53.8% (7 / 13) | Accuracy after: 53.8% (7 / 13) | Diff: 0.0%\n",
            "Image 14 / 10000 | Accuracy before: 57.1% (8 / 14) | Accuracy after: 57.1% (8 / 14) | Diff: 0.0%\n",
            "Image 15 / 10000 | Accuracy before: 53.3% (8 / 15) | Accuracy after: 53.3% (8 / 15) | Diff: 0.0%\n",
            "Image 16 / 10000 | Accuracy before: 56.2% (9 / 16) | Accuracy after: 56.2% (9 / 16) | Diff: 0.0%\n",
            "Image 17 / 10000 | Accuracy before: 52.9% (9 / 17) | Accuracy after: 52.9% (9 / 17) | Diff: 0.0%\n",
            "Image 18 / 10000 | Accuracy before: 50.0% (9 / 18) | Accuracy after: 50.0% (9 / 18) | Diff: 0.0%\n",
            "Image 19 / 10000 | Accuracy before: 52.6% (10 / 19) | Accuracy after: 52.6% (10 / 19) | Diff: 0.0%\n",
            "Image 20 / 10000 | Accuracy before: 55.0% (11 / 20) | Accuracy after: 55.0% (11 / 20) | Diff: 0.0%\n",
            "Image 21 / 10000 | Accuracy before: 52.4% (11 / 21) | Accuracy after: 52.4% (11 / 21) | Diff: 0.0%\n",
            "Image 22 / 10000 | Accuracy before: 50.0% (11 / 22) | Accuracy after: 54.5% (12 / 22) | Diff: 4.55%\n",
            "Image 23 / 10000 | Accuracy before: 47.8% (11 / 23) | Accuracy after: 56.5% (13 / 23) | Diff: 8.7%\n",
            "Image 24 / 10000 | Accuracy before: 50.0% (12 / 24) | Accuracy after: 58.3% (14 / 24) | Diff: 8.33%\n",
            "Image 25 / 10000 | Accuracy before: 52.0% (13 / 25) | Accuracy after: 60.0% (15 / 25) | Diff: 8.0%\n",
            "Image 26 / 10000 | Accuracy before: 53.8% (14 / 26) | Accuracy after: 61.5% (16 / 26) | Diff: 7.69%\n",
            "Image 27 / 10000 | Accuracy before: 51.9% (14 / 27) | Accuracy after: 59.3% (16 / 27) | Diff: 7.41%\n",
            "Image 28 / 10000 | Accuracy before: 53.6% (15 / 28) | Accuracy after: 60.7% (17 / 28) | Diff: 7.14%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-e17a78203070>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Calcola gli output delle immagini\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Combina le probabilità delle immagini\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         outputs = self.vit(\n\u001b[0m\u001b[1;32m    833\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    613\u001b[0m         )\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 )\n\u001b[1;32m    443\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;31m# in ViT, layernorm is also applied after self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# second residual connection is done here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Supponendo che load_dataset, classify_image, merged_labels e il modello siano definiti altrove\n",
        "accuracy_before = 0\n",
        "accuracy_after = 0\n",
        "\n",
        "# Load model (only once)\n",
        "model = load_model()\n",
        "original_model = deepcopy(model)\n",
        "\n",
        "data_loader = load_dataset(resize=False)\n",
        "\n",
        "# transformation = T.Compose([\n",
        "#         T.Resize((500, 500)),\n",
        "#         T.CenterCrop((384, 384)) ])\n",
        "\n",
        "transforms = v2.Compose([\n",
        "    v2.RandomResizedCrop(size=SIZE, antialias=True),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "resize_transformation = T.Compose([ T.Resize(SIZE) ])\n",
        "\n",
        "# Salva lo stato iniziale del modello\n",
        "initial_state = model.state_dict().copy()\n",
        "\n",
        "for index, img in enumerate(data_loader):\n",
        "    # Ripristina lo stato iniziale del modello\n",
        "    model = deepcopy(original_model)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "    # Azzera i gradienti prima di calcolare i nuovi\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Immagine ridimensionata (384x384)\n",
        "    img1 = resize_transformation(img[0])\n",
        "    # Immagini con augmentation\n",
        "    img2 = transforms(img[0])\n",
        "    img3 = transforms(img[0])\n",
        "    img4 = transforms(img[0])\n",
        "\n",
        "    # Concatena immagini\n",
        "    imgs = [img1, img2, img3, img4]\n",
        "    input = torch.cat(imgs, dim=0)\n",
        "\n",
        "    # Classificazione dell'immagine 1 prima delle augmentation\n",
        "    results = classify(model=model, img=img1)\n",
        "    results = elaborate_results(results=results)\n",
        "    predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "    # Aggiorna accuracy della classificazione senza augmentation\n",
        "    if img[1].item() == predicted[\"index\"]:\n",
        "        accuracy_before = accuracy_before + 1\n",
        "\n",
        "    predicted_before = predicted[\"label\"]\n",
        "\n",
        "    # Calcola gli output delle immagini\n",
        "    output = model(input.to(DEVICE))\n",
        "\n",
        "    # Combina le probabilità delle immagini\n",
        "    probabilities = torch.nn.functional.softmax(output.logits, dim=-1).squeeze().to(DEVICE)\n",
        "\n",
        "    # Calcolo entropia\n",
        "    marginal = torch.mean(probabilities, dim=0).to(DEVICE)\n",
        "    entropy = -torch.sum(marginal * torch.log(marginal)).to(DEVICE)\n",
        "    entropy.backward()\n",
        "\n",
        "    # Gradient step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Classificazione dell'immagine 1 dopo le augmentation\n",
        "    results = classify(model=model, img=img1)\n",
        "    results = elaborate_results(results=results)\n",
        "    predicted, results = results[\"predicted\"], results[\"results\"]\n",
        "\n",
        "    #print(probabilities1 == probabilities2)\n",
        "    # Aggiorna accuracy della classificazione con augmentation\n",
        "    if img[1].item() == predicted[\"index\"]:\n",
        "        accuracy_after = accuracy_after + 1\n",
        "\n",
        "    label1 = f\"Image {index + 1} / {len(data_loader)}\"\n",
        "    label2 = f\"Accuracy before: {round((accuracy_before / (index + 1)) * 100, 1)}% ({accuracy_before} / {index + 1})\"\n",
        "    label3 = f\"Accuracy after: {round((accuracy_after / (index + 1)) * 100, 1)}% ({accuracy_after} / {index + 1})\"\n",
        "    label4 = f\"Diff: {round((accuracy_after / (index + 1)) * 100 - (accuracy_before / (index + 1)) * 100, 2)}%\"\n",
        "\n",
        "    print(f\"{label1} | {label2} | {label3} | {label4}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}